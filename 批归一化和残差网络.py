'''
æ‰¹å½’ä¸€åŒ–
'''
'''
å¯¹è¾“å…¥çš„æ ‡å‡†åŒ–ï¼ˆæµ…å±‚æ¨¡å‹ï¼‰
å¤„ç†åçš„ä»»æ„ä¸€ä¸ªç‰¹å¾åœ¨æ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬ä¸Šçš„å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º1ã€‚
æ ‡å‡†åŒ–å¤„ç†è¾“å…¥æ•°æ®ä½¿å„ä¸ªç‰¹å¾çš„åˆ†å¸ƒç›¸è¿‘

æ‰¹é‡å½’ä¸€åŒ–ï¼ˆæ·±åº¦æ¨¡å‹ï¼‰
åˆ©ç”¨å°æ‰¹é‡ä¸Šçš„å€¼å’Œæ ‡å‡†å·®ï¼Œä¸æ–­è°ƒæ•´ç¥ç»ç½‘ç»œä¸­é—´è¾“å‡ºï¼Œä»è€Œä½¿æ•´ä¸ªç¥ç»ç½‘ç»œåœ¨å„å±‚çš„ä¸­é—´è¾“å‡ºçš„æ•°å€¼æ›´ç¨³å®šã€‚
1ã€å¯¹å…¨é“¾æ¥åšæ‰¹å½’ä¸€åŒ–
ä½ç½®ï¼šå…¨é“¾æ¥å±‚ä¸æ¿€æ´»å‡½æ•°ä¹‹é—´
å…¨è¿æ¥ï¼šğ‘¥=ğ‘Šğ‘¢+ğ‘   ğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡=ğœ™(ğ‘¥)
æ‰¹é‡å½’ä¸€åŒ–ï¼šå¯å­¦ä¹ å‚æ•° æ‹‰ä¼¸å‚æ•° å’Œåç§»å‚æ•° 

2.å¯¹å·ç§¯å±‚åšæ‰¹é‡å½’â¼€åŒ–
ä½ç½®ï¼šå·ç§¯è®¡ç®—ä¹‹åã€åº”â½¤æ¿€æ´»å‡½æ•°ä¹‹å‰ã€‚
å¦‚æœå·ç§¯è®¡ç®—è¾“å‡ºå¤šä¸ªé€šé“ï¼Œéœ€è¦å¯¹è¿™äº›é€šé“çš„è¾“å‡ºåˆ†åˆ«åšæ‰¹é‡å½’ä¸€åŒ–ï¼Œä¸”æ¯ä¸ªé€šé“éƒ½æ‹¥æœ‰ç‹¬ç«‹çš„æ‹‰ä¼¸å’Œåç§»å‚æ•°ã€‚ 
è®¡ç®—ï¼šå¯¹å•é€šé“ï¼Œbatchsize=m,å·ç§¯è®¡ç®—è¾“å‡º=pxq å¯¹è¯¥é€šé“ä¸­mÃ—pÃ—qä¸ªå…ƒç´ åŒæ—¶åšæ‰¹é‡å½’ä¸€åŒ–,ä½¿ç”¨ç›¸åŒçš„å‡å€¼å’Œæ–¹å·®ã€‚

3.é¢„æµ‹æ—¶çš„æ‰¹é‡å½’â¼€åŒ–
è®­ç»ƒï¼šä»¥batchä¸ºå•ä½,å¯¹æ¯ä¸ªbatchè®¡ç®—å‡å€¼å’Œæ–¹å·®ã€‚
é¢„æµ‹ï¼šç”¨ç§»åŠ¨å¹³å‡ä¼°ç®—æ•´ä¸ªè®­ç»ƒæ•°æ®é›†çš„æ ·æœ¬å‡å€¼å’Œæ–¹å·®ã€‚                    
'''

import torch
import torch.nn as nn
import d2lzh_pytorch as d2l
import torch.nn.functional as F

####################### æ‰¹é‡å½’ä¸€åŒ– ##################

def batch_norm(is_trainning,X,gamma,beta,moving_mean,moving_var,eps,monmentum):
    if not is_trainning:
        X_hat = (X-moving_mean)/torch.sqrt(moving_var+eps) #é¢„æµ‹åˆ™ç›´æ¥é©¶å…¥ç§»åŠ¨å¹³å‡å’Œæ–¹å·®
    else:
        assert len(X.shape) in [2,4] #å…¨ğŸ”— len(X.shape) = 2 å·ç§¯ï¼šlen(X.shap)e=4
        if (X.shape==2):
            mean = X.mean(dim =0)
            var = ((X - mean)**2).mean(dim=0)
        else:
            mean  = X.mean(dim=0,keepdim = True).mean(dim=2,keepdim = True).mean(dim=3,keepdim = True)
            var = ((X - mean)**2).mean(dim=0,keepdim = True).mean(dim=2,keepdim = True).mean(dim =3,keepdim = True)
        X_hat = (X -mean)/torch.sqrt(var+eps)
        #æ›´æ–°ç§»åŠ¨å¹³å‡çš„å‡å€¼å’Œæ–¹å·®
        moving_mean = monmentum*moving_mean+(1-monmentum)*mean
        moving_var = monmentum*var +(1-monmentum)*var
    Y = gamma* X_hat +beta #æ‹‰å‡å’Œåç§»
    return Y,moving_mean,moving_var

class BatchNorm(nn.Module):
    def __init__(self,num_features,num_dims):
        super(BatchNorm,self).__init__()
        #nums_feature BNçš„ç‹¬ç«‹ç»´åº¦
        if num_dims ==2:
            shape = (1,num_features) #å…¨é“¾æ¥è¾“å‡ºç¥ç»å…ƒ
        else:
            shape =(1,num_features,1,1) #é€šé“æ•°
        #æ‹‰ä¼¸ åç§»å‚ä¸æ¢¯åº¦è®¡ç®— åˆå§‹åŒ–0 1
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.zeros(shape)

    def forward(self,X):
        # è‹¥Xä¸åœ¨å†…å­˜ å°†moving_mean moving_var å¤åˆ¶åˆ°Xæ‰€åœ¨çš„æ˜¾å­˜
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        Y ,self.moving_mean,self.moving_var = \
            batch_norm(self.training,X,self.gamma,self.beta,self.moving_mean,self.moving_var,
                   eps=1e-5,monmentum=0.9)
        return Y

#åŸºäºLeNetåº”ç”¨
net = nn.Sequential(
    nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5),
    BatchNorm(6,num_dims =4),
    #nn.BatchNorm2d(6)
    nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2,padding=2),
    nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5),
    BatchNorm(16,num_dims=4),
    #nn.BatchNorm2d(16),
    nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2,padding=2),
    d2l.FlattenLayer(),
    nn.Linear(16*4*4,120),
    BatchNorm(120,num_dims=2),
    # nn.BatchNorm1d(120)
    nn.Sigmoid(),
    nn.Linear(120,84),
    BatchNorm(84,num_dims=2),
    # nn.BatchNorm1d(84)
    nn.Sigmoid(),
    nn.Linear(84,10)
)

print(net)

torch.optim.Adam(net.parameters(),lr = 0.1)

####################### ResNet ##################
'''
è®¾å®šè¾“å‡ºé€šé“ï¼Œæ˜¯å¦ä½¿ç”¨é¢å¤–çš„1x1å·ç§¯å±‚ä¿®æ”¹é€šé“æ•°
è‹¥ residual ä¸­in_channel å’Œout_channelç›¸ç­‰ åˆ™ä¸ä½¿ç”¨1x1å·ç§¯æ”¹å˜é€šé“ ç›´æ¥ä¸è¾“å…¥çš„Tensor X ç›¸åŠ 
è‹¥ä¸åŒ åˆ™ä½¿ç”¨1x1å·ç§¯ æ”¹å˜é€šé“
Residual å¯ç›´æ¥åœ¨d2lä¸­è°ƒç”¨
'''
#æ„å»ºé¤å‰ç½‘ç»œ
class Residual(nn.Module):
    def __init__(self,in_channels,out_channels,use_1x1conv =False,stride =1):
        super(Residual,self).__init__()
        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1,stride=stride)
        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, X):
        X1 = F.relu(self.bn1(self.conv1(X)))
        Y= self.bn2((self.conv2(X1)))
        #1x1 å·ç§¯è¾“æ”¹å˜é€šé“æ•°
        if self.conv3:
             X = self.conv3(X) + Y
        #shortcut ç›¸åŠ 
        return F.relu(X + Y)

#in_channel = out_channel
X = torch.randn((4,3,6,6))
Res_blk1 = Residual(3,3) #äº§ç”Ÿå¯¹è±¡
print(Res_blk1(X).shape) #torch.Size(4,3,6,6)

#in_channel != out_channel
Res_blk2 = Residual(3,6,use_1x1conv=True,stride=2)
print(Res_blk2(X).shape) #torch.Size(4,6,3,3)

'''
å·ç§¯(64,7x7,3)
æ‰¹é‡ä¸€ä½“åŒ–
æœ€å¤§æ± åŒ–(3x3,2)
æ®‹å·®å— x4 (æ­¥å¹…ä¸º2çš„æ®‹å·®å— ä¹‹é—´å‡å°é«˜å’Œå®½)
å…¨å±€å¹³å‡æ± åŒ–
å…¨è¿æ¥
'''

def ResNet():
    net = nn.Sequential(
        nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),
        nn.BatchNorm2d(64),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2,stride=2,padding=1)
    )

    #å †ç Œæ®‹å·®ç½‘ç»œ
    def resnet_block(in_channels,out_channels,num_residuals,first_block=False):
        if first_block:
            assert in_channels == out_channels
        blk = []
        for i in range(num_residuals):
            if i==0 and not first_block:
                blk.append(Residual(in_channels,out_channels,use_1x1conv=True,stride=2))
            else:
                blk.append(Residual(out_channels,out_channels))
        return nn.Sequential(*blk)

    net.add_module('resnet_block1',resnet_block(64,64,2,first_block=True))
    net.add_module('resnet_block2',resnet_block(64,128,2)) #é€šé“å¢åŠ  å›¾åƒå¤§å°å‡åŠ
    net.add_module('resnet_block3',resnet_block(128,256,2))
    net.add_module('resnet_block4',resnet_block(256,512,2))

    net.add_module('gloval_ave_pool',d2l.GlobalAvgPool2d()) #è¾“å‡º (batch,512,1,1)
    net.add_module('fc',nn.Sequential(d2l.FlattenLayer(),
                                      nn.Linear(512,10)))

    return net

Rsnet = ResNet()
X = torch.rand(1,1,224,224)
for name,layer in Rsnet.named_children():
    X = layer(X)
    print(name,' output shape: ',X.shape)

####################### ç¨ å¯†è¿æ¥ç½‘ç»œ(DenseNet) ##################

#ç¨ å¯†å—(dense block) å®šä¹‰äº†è¾“å…¥å’Œè¾“å‡ºå¦‚ä½•è¿ç»“
#è¿‡åº¦å±‚(transition layer) ç”¨æ¥æ§åˆ¶é€šé“æ•°
#ResNetåœ¨è·¨å±‚è¿æ¥ç›¸åŠ  ä¸ DenseNet è·¨å±‚è¿æ¥è¿æ¥

class DenseBlock(nn.Module):
    def __init__(self,num_convs,in_channels,out_channels):
        super(DenseBlock,self).__init__()
        net = []
        for i in range(num_convs):
             in_c = in_channels + i*out_channels
             net.append(self.conv_block(in_c,out_channels))
        self.net = nn.ModuleList(net)
        self.out_channels = in_channels + num_convs*out_channels

    #å®šä¹‰å·ç§¯å±‚
    def conv_block(self,in_channels, out_channels):
        blk = nn.Sequential(nn.BatchNorm2d(in_channels),
                            nn.ReLU(),
                            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
        return blk

    def forward(self, X):
        for blk in self.net:
            Y =  blk(X) #æ¯ä¸€ä¸ªblkè¾“å…¥é€šé“æ•°é€’å¢
            X = torch.cat([X,Y],dim = 1) #é€šé“ç»´åº¦ä¸Šå°†è¾“å…¥å’Œè¾“å‡ºè¿æ¥
        return X

blk = DenseBlock(2,3,10)
X = torch.rand(4,3,8,8)
Y = blk(X)
print(Y.shape) # torch.Size([4,23,8,8])

#è¿‡åº¦å±‚
# 1x1å·ç§¯ï¼šå‡å°‘é€šé“æ•°
# æ­¥å¹…ä¸º2çš„å¹³å‡æ± åŒ–å±‚ï¼šå‡åŠé«˜å’Œå®½
def transition_block(in_channels,out_channels):
    blk = nn.Sequential(
        nn.BatchNorm2d(in_channels),
        nn.ReLU(),
        nn.Conv2d(in_channels,out_channels,kernel_size=1),
        nn.AvgPool2d(kernel_size=2,stride=2)
    )
    return blk


blk = transition_block(23,10)
print(blk(Y).shape) # torch.Size([4, 10, 4, 4])

#DenseNet æ¨¡å‹ ç¨ å¯†å±‚+è¿‡æ¸¡å±‚
def DenseNet():
    #åˆå§‹æ¨¡å—
    net = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

    num_channels,growth_rate = 64,32
    num_convs_in_dense_blocks = [4,4,4,4]
    # ç¨ å¯†å— + è¿‡æ¸¡å—
    for i,num_convs in enumerate(num_convs_in_dense_blocks):
            DenseBlk= DenseBlock(num_convs,num_channels,growth_rate)
            net.add_module('DenseBlock_%d'%i,DenseBlk)
            # ç¨ å¯†å—çš„è¾“å‡ºé€šé“ä½œä¸ºè¿‡æ¸¡å±‚çš„è¾“å…¥
            num_channels = DenseBlk.out_channels
            #åœ¨ç¨ å¯†å—(é€šé“æ•°å¢åŠ ) ä¹‹é—´åŠ å…¥è¿‡åº¦å±‚(å›¾åƒå¤§å°å‡åŠï¼Œé€šé“æ•°å‡åŠ)
            if i!=len(num_convs_in_dense_blocks) -1:
                TransBlk= transition_block(num_channels,num_channels//2)
                net.add_module('Trasition_block_%d'%i,TransBlk)
                num_channels = num_channels//2

    net.add_module('BN',nn.BatchNorm2d(num_channels))
    net.add_module('relu',nn.ReLU())
    #GlobalAvgPool2d è¾“å‡º (Batch,num_channels,1,1)
    net.add_module('global_avg_pool',d2l.GlobalAvgPool2d())
    net.add_module('fc',nn.Sequential(d2l.FlattenLayer(),
                                      nn.Linear(num_channels,10)))
    return net


def queryLayer(net):
    X= torch.rand(1,1,96,96)
    for name, layer in net.named_children():
        X = layer(X)
        print(name,'output shape:\t',X.shape)


DenseN = DenseNet()
queryLayer(DenseN)


def train():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size = 16
    # å¦‚å‡ºç°â€œout of memoryâ€çš„æŠ¥é”™ä¿¡æ¯ï¼Œå¯å‡å°batch_sizeæˆ–resize
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root='Data/softmax/FashionMNIST2065')
    lr, num_epochs = 0.001, 5
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    d2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)